{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loose-contrary",
   "metadata": {},
   "source": [
    "# HW07 Векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-begin",
   "metadata": {},
   "source": [
    "## 1 Векторизуем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olive-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    \n",
    "    tokens = [token.text for token in list(razdel_tokenize(text))]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sized-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compound-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "english-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>право полицейский проникновение жилище решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>-1</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "2  Президент Египта ввел чрезвычайное положение в...   \n",
       "3  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2 label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...     0   \n",
       "1  Правила внесудебного проникновения полицейских...     0   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...     0   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...    -1   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...     0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилище решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешить стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  \n",
       "2  власть египет угрожать ввести страна чрезвычай...  \n",
       "3      самолёт мчс вывезти россиянин разрушить сирия  \n",
       "4      самолёт мчс вывезти россиянин разрушить сирия  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "received-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta = open('lenta.txt').read().splitlines()[:1000]\n",
    "data_lenta = [normalize(text) for text in lenta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "amino-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "русский инвалид 16 сентябрь 1914 года.министерство народный просвещение вид происходить чрезвычайный событие признать соответственный день годовщина день рождение м.ю лермонтов 2-й октябрь 1914 год ограничиться совершение учебный заведение панихида поэт отложить празднование юбилей благоприятный время\n"
     ]
    }
   ],
   "source": [
    "print(data_lenta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "grateful-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_w2v = gensim.models.Word2Vec([text.split() for text in data_lenta], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "residential-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_w2v = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "southern-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "civic-funeral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.text_1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "brilliant-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-34c80056d91d>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = model[word]\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "X_1_my_w2v = np.zeros((len(data.text_1_norm), dim))\n",
    "X_2_my_w2v = np.zeros((len(data.text_2_norm), dim))\n",
    "\n",
    "for i, text in enumerate(data.text_1_norm.values):\n",
    "    X_1_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data.text_2_norm.values):\n",
    "    X_2_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
    "\n",
    "X_my_w2v = np.concatenate([X_1_my_w2v, X_2_my_w2v], axis=1)\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "hairy-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_1_ready_w2v = np.zeros((len(data.text_1_norm), dim))\n",
    "X_2_ready_w2v = np.zeros((len(data.text_2_norm), dim))\n",
    "\n",
    "for i, text in enumerate(data.text_1_norm.values):\n",
    "    X_1_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data.text_2_norm.values):\n",
    "    X_2_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
    "\n",
    "X_ready_w2v = np.concatenate([X_1_ready_w2v, X_2_ready_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "together-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dynamic-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(C=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aerial-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44329184 0.45159059 0.48650519 0.38823529 0.41038062]\n",
      "[0.40940526 0.40940526 0.40899654 0.40899654 0.40899654]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf_lr, X_my_w2v, y, scoring=\"f1_micro\"))\n",
    "print(cross_val_score(clf_lr, X_ready_w2v, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-fighter",
   "metadata": {},
   "source": [
    "Обе модели справляются достаточно одинаково"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-maker",
   "metadata": {},
   "source": [
    "## 2. Преобразуем тексты в векторы в каждой паре 5 методами - SVD, NMF, Word2Vec (свой и русвекторовский), Fastext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "tough-cambodia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.4, max_features=1000, min_df=3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "tfidf.fit(pd.concat([data.text_1_norm, data.text_2_norm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "democratic-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(200)\n",
    "\n",
    "X_1_svd = svd.fit_transform(tfidf.transform(data.text_1_norm))\n",
    "X_2_svd = svd.fit_transform(tfidf.transform(data.text_2_norm))\n",
    "\n",
    "X_svd = np.concatenate([X_1_svd, X_2_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "backed-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout _\n",
    "nmf = NMF(100)\n",
    "\n",
    "X_1_nmf = nmf.fit_transform(tfidf.transform(data.text_1_norm))\n",
    "X_2_nmf = nmf.fit_transform(tfidf.transform(data.text_2_norm))\n",
    "\n",
    "X_nmf = np.concatenate([X_1_nmf, X_2_nmf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "mental-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-34c80056d91d>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = model[word]\n"
     ]
    }
   ],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in data_lenta], size=50, min_n=4, max_n=8) \n",
    "\n",
    "X_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_2_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "X_ft = np.concatenate([X_1_ft, X_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "inside-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "prospective-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_w2v_dist = cosine_distances(X_1_my_w2v, X_2_my_w2v)\n",
    "ready_w2v_dist = cosine_distances(X_1_ready_w2v, X_2_ready_w2v)\n",
    "svd_dist = cosine_distances(X_1_svd, X_2_svd)\n",
    "nmf_dist = cosine_distances(X_1_nmf, X_2_nmf)\n",
    "ft_dist = cosine_distances(X_1_ft, X_2_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "funny-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.concatenate((my_w2v_dist, ready_w2v_dist, svd_dist, nmf_dist, ft_dist), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "naval-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2_lr = LogisticRegression(C=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "clear-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41839557 0.44190871 0.45190311 0.38961938 0.40968858]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf_lr, distances, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-store",
   "metadata": {},
   "source": [
    "Считался намного дольше, но результат стал не сильно лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-second",
   "metadata": {},
   "source": [
    "#### Изменим параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "streaming-leonard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, max_features=1500, min_df=5)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, max_df=0.9, max_features=1500)\n",
    "tfidf.fit(pd.concat([data.text_1_norm, data.text_2_norm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "becoming-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(400)\n",
    "\n",
    "X_1_svd = svd.fit_transform(tfidf.transform(data.text_1_norm))\n",
    "X_2_svd = svd.fit_transform(tfidf.transform(data.text_2_norm))\n",
    "\n",
    "X_svd = np.concatenate([X_1_svd, X_2_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "amateur-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout _\n",
    "nmf = NMF(200)\n",
    "\n",
    "X_1_nmf = nmf.fit_transform(tfidf.transform(data.text_1_norm))\n",
    "X_2_nmf = nmf.fit_transform(tfidf.transform(data.text_2_norm))\n",
    "\n",
    "X_nmf = np.concatenate([X_1_nmf, X_2_nmf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "psychological-bullet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-34c80056d91d>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = model[word]\n"
     ]
    }
   ],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in data_lenta], size=50, min_n=4, max_n=8) \n",
    "\n",
    "dim = 150\n",
    "\n",
    "X_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_2_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "X_ft = np.concatenate([X_1_ft, X_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "colored-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-34c80056d91d>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  v = model[word]\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "\n",
    "X_1_my_w2v = np.zeros((len(data.text_1_norm), dim))\n",
    "X_2_my_w2v = np.zeros((len(data.text_2_norm), dim))\n",
    "\n",
    "for i, text in enumerate(data.text_1_norm.values):\n",
    "    X_1_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data.text_2_norm.values):\n",
    "    X_2_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
    "\n",
    "X_my_w2v = np.concatenate([X_1_my_w2v, X_2_my_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "floppy-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 400\n",
    "X_1_ready_w2v = np.zeros((len(data.text_1_norm), dim))\n",
    "X_2_ready_w2v = np.zeros((len(data.text_2_norm), dim))\n",
    "\n",
    "for i, text in enumerate(data.text_1_norm.values):\n",
    "    X_1_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data.text_2_norm.values):\n",
    "    X_2_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
    "\n",
    "X_ready_w2v = np.concatenate([X_1_ready_w2v, X_2_ready_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "thorough-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.concatenate((my_w2v_dist, ready_w2v_dist, svd_dist, nmf_dist, ft_dist), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "moderate-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2_lr = LogisticRegression(C=3000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "brave-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41839557 0.44190871 0.45190311 0.38961938 0.40968858]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout _\n",
    "print(cross_val_score(clf_lr, distances, y, scoring=\"f1_micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-dictionary",
   "metadata": {},
   "source": [
    "Считалось ещё дольше, но результат опять не стал лучше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
