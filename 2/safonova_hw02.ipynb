{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# домашнее задание №2\n",
    "## Анастасия Сафонова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('data/lenta.txt').read()[:300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сначала сгенерируем биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news_bi = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news_bi:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news_bi = np.zeros((len(unigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in bigrams_news:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_news_bi[word2id_news[word1]][word2id_news[word2]] =  (bigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bi(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "таких автомобилей ford терри бресниан сообщил корреспонденту итар-тасс в печатниках московский гарнизон милиции \n",
      " в силу советских танков введенных в случае победила \n",
      " полтора года указом президента фирмы mabetex были однако не принял экстренные меры не исключил фактор мщения со ссылкой на земле колыхаясь знамена шумели и с премьер-министром японии еще больше у мвф \n",
      " земля на 10 лет представители американских банках \n",
      " национальная авиакомпания олимпик предложила бесплатные билеты для нормальной работы \n",
      " когда тот думает о статусе иерусалима о себе в дагестане министр внутренних и методами \n",
      " велению рока послушный противник \n",
      " сразу на ленинском проспекте юго-западный администивный округ\n"
     ]
    }
   ],
   "source": [
    "print(generate_bi(matrix_news_bi, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь сгенерируем триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news_tri = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news_tri:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news_tri = np.zeros((len(bigrams_news), len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    first_word, second_word, third_word = ngram.split()\n",
    "    key = ' '.join([first_word, second_word])\n",
    "    matrix_news_tri[bigram2id_news[key]][word2id_news[third_word]] = (trigrams_news[ngram]/bigrams_news[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tri(matrix, id2word, word2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    first_word, second_word = start.split()\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = word2id[start]\n",
    "            first_word, second_word = start.split()\n",
    "        else:\n",
    "            next_word = ' '.join([second_word, id2word[chosen]])\n",
    "            current_idx = word2id[next_word]\n",
    "            second_word = id2word[chosen]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "коржаков напомнил что в настоящее время в главной миссии оон передаваемым риа новости руководитель пресс-службы мвд по москве и московской области \n",
      " 9 сентября не будет больше совершать теракты против турции сообщает bbc президент азербайджана гейдар алиев поручил разработатку и создание газопровода для доставки природного газа за рубеж по каналам bank of new york якобы связанные с борисом ельциным \n",
      " в ликвидации последствий взрыва пострадали получив различные травмы \n",
      " этот архив был доставлен в военный госпиталь на территории россии и чечни он сообщил также что не существует каких-либо плановограничить степень информированности людей о событиях на северном кавказе обвиняя во всем\n"
     ]
    }
   ],
   "source": [
    "print(generate_tri(matrix_news_tri, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст, сгенирированный по триграммам, выглядит чуть более осмысленным, чем тот, что создан по биграммам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Напишите функцию оценивания нграммов на основе PMI. Используйте эту функцию вместо дефолтной в gensim.models.Phrases Обучите два последовательных модели Phrases с такой функцией и проанализируйте результаты, получаемые после преобразования текстов двумя Phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('russian') + [\"это\", \"весь\"])\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = re.findall('[а-яёa-z0-9]+', text.lower())\n",
    "    normalized_text = [morph.parse(word)[0].normal_form for word \\\n",
    "                                                            in tokens]\n",
    "    normalized_text = [word for word in normalized_text if len(word) > 2 and word not in stops]\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, stops, n=2):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/lenta.txt').read()[:500000]\n",
    "corpus = preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\n",
    "    try:\n",
    "        score = np.log((bigram_count / corpus_word_count) / ((worda_count / corpus_word_count) * (wordb_count / corpus_word_count)))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить_взрыв_произойти_третий',\n",
       " 'уровнечетвертый_ярус_комплекс_зал',\n",
       " 'игровой_автомат']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=0)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=0)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить_взрыв_произойти_третий',\n",
       " 'уровнечетвертый_ярус_комплекс_зал',\n",
       " 'игровой_автомат']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=3)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=3)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти_третий_уровнечетвертый',\n",
       " 'ярус_комплекс_зал_игровой',\n",
       " 'автомат']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=5)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=5)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти',\n",
       " 'третий_уровнечетвертый_ярус',\n",
       " 'комплекс',\n",
       " 'зал_игровой_автомат']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=8)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=8)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти',\n",
       " 'третий',\n",
       " 'уровнечетвертый_ярус',\n",
       " 'комплекс',\n",
       " 'зал',\n",
       " 'игровой',\n",
       " 'автомат']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=10)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=10)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти',\n",
       " 'третий',\n",
       " 'уровнечетвертый',\n",
       " 'ярус',\n",
       " 'комплекс',\n",
       " 'зал',\n",
       " 'игровой',\n",
       " 'автомат']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=13)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=13)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)\n",
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### я не уверена, что сделала это задание правильно, потому что результат получается очень странный. Вероятно, лучший threshold должен быть 8 или 10, тогда получается хоть что-то отдаленно напоминающее коллокации. Если он больше, то всё распадается на одиночные слова, если меньше -- будто бы рандомно соединяютюся"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
